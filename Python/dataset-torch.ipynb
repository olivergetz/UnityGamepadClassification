{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "404bcd48",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipynb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmetrics\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mipynb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfull\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mearly_stopper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ipynb'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import torch\n",
    "from torch import nn\n",
    "from datetime import datetime\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import torchmetrics as metrics\n",
    "from ipynb.fs.full.early_stopper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c1337cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed_value=4052):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def feature_extraction(data, sr=100, window_length = 256, hop_length = 64):\n",
    "    \n",
    "    extracted_features_frame = pd.DataFrame()\n",
    "    for col in data.columns:\n",
    "        zcr = librosa.feature.zero_crossing_rate(y=np.array(data[col], dtype=np.float64),\n",
    "                                                                        frame_length=window_length, \n",
    "                                                                        hop_length=hop_length)\n",
    "    \n",
    "        rms = librosa.feature.rms(y=np.array(data[col], dtype=np.float64),\n",
    "                                                        frame_length=window_length, \n",
    "                                                        hop_length=hop_length)\n",
    "        \n",
    "        extracted_features_frame[col + \" RMS\"] = rms.squeeze()\n",
    "        #extracted_features_frame[col + \" ZCR\"] = zcr.squeeze()\n",
    "    \n",
    "    return extracted_features_frame\n",
    "\n",
    "def save_metrics(metrics_path:str,date_and_time,model_name:str,time,\n",
    "                 data_size:int,seed:int, metrics_dict:dict):\n",
    "    # Info common to all metrics\n",
    "    meta_data = {\n",
    "    'Date Time': date_and_time,\n",
    "    'Name': model_name, \n",
    "    'Training Time': total_training_time,\n",
    "    'Dataset Size': data_size,\n",
    "    'Seed': seed\n",
    "    }\n",
    "    \n",
    "    # Merge MetaModel-specific metrics\n",
    "    metrics_dict = meta_data | metrics_dict\n",
    "    \n",
    "    # Convert to data frame\n",
    "    metrics_frame = pd.DataFrame(columns=metrics_dict.keys())\n",
    "\n",
    "    # Check if metrics csv exists\n",
    "    metrics_frame = pd.DataFrame(metrics_dict, index=[0])\n",
    "\n",
    "    if (os.path.exists(metrics_path)):\n",
    "        df = pd.read_csv(metrics_path, index_col=0)\n",
    "        df = pd.concat([df, metrics_frame], ignore_index=True)\n",
    "        df.to_csv(metrics_path)\n",
    "\n",
    "    else:\n",
    "        metrics_frame.to_csv(metrics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4fb8211",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/\"\n",
    "\n",
    "# PRE PROCESSING\n",
    "\n",
    "# These are irrelevant for the current task due to being idle. We remove them to speed up the training process.\n",
    "# Unity's Barracuda can only take tensors of size 8 or below, so more filtering must be done.\n",
    "cols_to_ignore = [\"Timestamp\", \"D-Pad\", \"Touch\", \"L3\", \"R3\", \"L1\", \"R1\", \"L2\",\n",
    "       'Button North', 'Button East', 'Button South', 'Button West', 'R2']\n",
    "\n",
    "# Filter out slow start and finish, ensure same size.\n",
    "start_slice = 500\n",
    "end_slice = 5500\n",
    "\n",
    "labels_dict = {\n",
    "    0 : \"idle\",\n",
    "    1 : \"low_activity\",\n",
    "    2 : \"medium_activity\",\n",
    "    3 : \"high_activity\"\n",
    "}\n",
    "\n",
    "# Raw Data\n",
    "idle_frame_raw = pd.read_csv(data_path + \"idle_exaggerated.csv\").iloc[start_slice:end_slice]\n",
    "idle_labels_raw = np.zeros(idle_frame_raw.shape[0])\n",
    "\n",
    "low_activity_frame_raw = pd.read_csv(data_path + \"low_activity_exaggerated.csv\").iloc[start_slice:end_slice]\n",
    "low_activity_labels_raw = np.zeros(low_activity_frame_raw.shape[0]) + 1\n",
    "\n",
    "medium_activity_frame_raw = pd.read_csv(data_path + \"medium_activity_exaggerated.csv\").iloc[start_slice:end_slice]\n",
    "medium_activity_labels_raw = np.zeros(medium_activity_frame_raw.shape[0]) + 2\n",
    "\n",
    "high_activity_frame_raw = pd.read_csv(data_path + \"high_activity_exaggerated.csv\").iloc[start_slice:end_slice]\n",
    "high_activity_labels_raw = np.zeros(high_activity_frame_raw.shape[0]) + 3\n",
    "\n",
    "# Feature Extract\n",
    "idle_frame_features = feature_extraction(idle_frame_raw)\n",
    "idle_labels_features = np.zeros(idle_frame_features.shape[0])\n",
    "\n",
    "low_activity_frame_features = feature_extraction(low_activity_frame_raw)\n",
    "low_activity_labels_features = np.zeros(low_activity_frame_features.shape[0]) + 1\n",
    "\n",
    "medium_activity_frame_features = feature_extraction(medium_activity_frame_raw)\n",
    "medium_activity_labels_features = np.zeros(medium_activity_frame_features.shape[0]) + 2\n",
    "\n",
    "high_activity_frame_features = feature_extraction(high_activity_frame_raw)\n",
    "high_activity_labels_features = np.zeros(high_activity_frame_features.shape[0]) + 3\n",
    "\n",
    "# Concatenate Raw Dataset\n",
    "labels_raw = np.concatenate((idle_labels_raw, low_activity_labels_raw, medium_activity_labels_raw, high_activity_labels_raw))\n",
    "data_raw = pd.concat((idle_frame_raw, low_activity_frame_raw, medium_activity_frame_raw, high_activity_frame_raw))\n",
    "data_raw.reset_index(drop=True, inplace=True)\n",
    "# Concatenate Extracted Features Dataset\n",
    "labels_features = np.concatenate((idle_labels_features, low_activity_labels_features, medium_activity_labels_features, high_activity_labels_features))\n",
    "data_features = pd.concat((idle_frame_features, low_activity_frame_features, medium_activity_frame_features, high_activity_frame_features))\n",
    "data_features.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Remove unwanted features\n",
    "for col in data_raw.columns:\n",
    "    for header in cols_to_ignore:\n",
    "        if header in col:\n",
    "            data_raw = data_raw.drop(col, axis=1)\n",
    "            \n",
    "for col in data_features.columns:\n",
    "    for header in cols_to_ignore:\n",
    "        if header in col:\n",
    "            data_features = data_features.drop(col, axis=1)\n",
    "            \n",
    "            \n",
    "# There is a leading whitespace in each header. This removes it.\n",
    "for col in data_raw.columns:\n",
    "    if (col[0] == \" \"):\n",
    "        data_raw.rename(columns={col: col.strip()}, inplace=True)\n",
    "        data_features.rename(columns={col: col.strip()}, inplace=True)\n",
    "        \n",
    "for col in data_features.columns:\n",
    "    if (col[0] == \" \"):\n",
    "        data_features.rename(columns={col: col.strip()}, inplace=True)\n",
    "        \n",
    "        \n",
    "data_features.to_csv(\"./data/data_features.csv\")\n",
    "data_raw.to_csv(\"./data/data_raw.csv\")\n",
    "pd.Series(labels_features).to_csv(\"./data/labels_features.csv\")\n",
    "pd.Series(labels_raw).to_csv(\"./data/labels_raw.csv\")\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fad63412",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 4052\n",
    "training_size = 0.9\n",
    "seed_everything(seed_value=seed)\n",
    "# Convert to tensor\n",
    "data_raw_tensor = torch.tensor([data_raw[col].astype('float64') for col in data_raw.columns]).T.double()\n",
    "lab_raw_tensor = torch.tensor(labels_raw).double()\n",
    "\n",
    "data_features_tensor = torch.tensor([data_features[col].astype('float64') for col in data_features.columns]).T.double()\n",
    "lab_features_tensor = torch.tensor(labels_features).double()\n",
    "\n",
    "# For training using features\n",
    "feat_data_train, feat_data_test, feat_lab_train, feat_lab_test = sklearn.model_selection.train_test_split(data_features_tensor,\n",
    "                                                                                                        lab_features_tensor,\n",
    "                                                                                                        train_size=training_size, \n",
    "                                                                                                        random_state=seed,\n",
    "                                                                                                        stratify=labels_features)  \n",
    "\n",
    "# For training using raw data\n",
    "raw_data_train, raw_data_test, raw_lab_train, raw_lab_test = sklearn.model_selection.train_test_split(data_raw_tensor,\n",
    "                                                                        lab_raw_tensor,\n",
    "                                                                        train_size=training_size, \n",
    "                                                                        random_state=seed, \n",
    "                                                                        stratify=labels_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "91162eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\tLoss: 1.627218246459961  \tTraining accuracy: 49.3%\tValidation accuracy: 50.0%\n",
      "epoch: 1\tLoss: 1.0110665559768677  \tTraining accuracy: 50.0%\tValidation accuracy: 50.0%\n",
      "epoch: 2\tLoss: 0.21911858022212982  \tTraining accuracy: 96.1%\tValidation accuracy: 100.0%\n",
      "epoch: 3\tLoss: 0.0006300609675236046  \tTraining accuracy: 92.3%\tValidation accuracy: 93.8%\n",
      "epoch: 4\tLoss: 0.0018195039592683315  \tTraining accuracy: 91.9%\tValidation accuracy: 93.8%\n",
      "epoch: 5\tLoss: 0.019162412732839584  \tTraining accuracy: 93.0%\tValidation accuracy: 100.0%\n",
      "epoch: 6\tLoss: 0.021619249135255814  \tTraining accuracy: 96.1%\tValidation accuracy: 100.0%\n",
      "epoch: 7\tLoss: 0.4661857783794403  \tTraining accuracy: 96.8%\tValidation accuracy: 100.0%\n",
      "epoch: 8\tLoss: 0.04201429337263107  \tTraining accuracy: 97.2%\tValidation accuracy: 96.9%\n",
      "epoch: 9\tLoss: 0.01610625721514225  \tTraining accuracy: 85.9%\tValidation accuracy: 84.4%\n",
      "epoch: 10\tLoss: 0.01856253109872341  \tTraining accuracy: 96.5%\tValidation accuracy: 100.0%\n",
      "epoch: 11\tLoss: 0.0010305099422112107  \tTraining accuracy: 95.4%\tValidation accuracy: 100.0%\n",
      "epoch: 12\tLoss: 0.00661161495372653  \tTraining accuracy: 97.2%\tValidation accuracy: 100.0%\n",
      "epoch: 13\tLoss: 0.0029037713538855314  \tTraining accuracy: 99.3%\tValidation accuracy: 100.0%\n",
      "epoch: 14\tLoss: 0.003951955586671829  \tTraining accuracy: 99.3%\tValidation accuracy: 100.0%\n",
      "epoch: 15\tLoss: 0.0029900625813752413  \tTraining accuracy: 98.9%\tValidation accuracy: 100.0%\n",
      "epoch: 16\tLoss: 0.005612685810774565  \tTraining accuracy: 99.3%\tValidation accuracy: 100.0%\n",
      "epoch: 17\tLoss: 0.0018798314267769456  \tTraining accuracy: 99.3%\tValidation accuracy: 100.0%\n",
      "epoch: 18\tLoss: 0.0021058782003819942  \tTraining accuracy: 99.3%\tValidation accuracy: 100.0%\n",
      "epoch: 19\tLoss: 0.0018254535971209407  \tTraining accuracy: 99.3%\tValidation accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Using extracted features\n",
    "model_name = \"dsc_torch_nn_rms_exaggerated\"\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "\n",
    "def train(model, x, y, optimizer, scheduler):\n",
    "    model.train()\n",
    "    for feature_vector, label_true in zip(x, y):\n",
    "        optimizer.zero_grad()\n",
    "        label_pred = model(feature_vector)\n",
    "        loss = nn.functional.cross_entropy(label_pred, label_true.long())\n",
    "        #loss_fn = nn.MSELoss(reduction='mean')\n",
    "        #loss = loss_fn(label_pred, label_true)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, x, y):\n",
    "    model.eval()\n",
    "    labels_true, predictions = [], []\n",
    "    for feature_vector, label_true in zip(x, y):\n",
    "        output = model(feature_vector)\n",
    "        predictions.append(output.argmax().tolist())\n",
    "        labels_true.append(label_true.tolist())\n",
    "\n",
    "    #mca = MulticlassAccuracy(num_classes=4, average='macro')\n",
    "    #return mca(torch.FloatTensor(predictions), torch.IntTensor(labels_true))\n",
    "    return (torch.tensor(predictions) == torch.tensor(labels_true)).float().mean() * 100.0\n",
    "    \n",
    "class DSC_Classifier(nn.Module):\n",
    "    def __init__(self, n_feats, n_labels, n_hidden_layers, hidden_dims, dropout):\n",
    "        super().__init__()\n",
    "        self.input = nn.Linear(n_feats, hidden_dims, bias=True)\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_dims, hidden_dims) \n",
    "            for _ in range(n_hidden_layers)\n",
    "        ])\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dims, n_labels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input(x).relu()\n",
    "        for layer in self.hidden_layers:\n",
    "            x = x + layer(x).relu()\n",
    "            \n",
    "        logits = self.output(x)\n",
    "        return logits\n",
    "\n",
    "epochs = 20\n",
    "hidden_layers = 3\n",
    "hidden_dims = 7\n",
    "dropout = 0.2\n",
    "learning_rate = 0.00001\n",
    "gamma=0.9\n",
    "use_raw = False;\n",
    "\n",
    "if (use_raw):\n",
    "    model = DSC_Classifier(len(raw_data_train[1]), len(labels_dict), hidden_layers, hidden_dims, dropout) # 16000, 8, 4\n",
    "else:\n",
    "    model = DSC_Classifier(len(feat_data_train[1]), len(labels_dict), hidden_layers, hidden_dims, dropout) # 128, 8, 4\n",
    "\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.0)\n",
    "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr = 0.01, epochs=epochs, steps_per_epoch=312)\n",
    "\n",
    "start_time_training = time.time()\n",
    "for epoch in range(epochs):\n",
    "    if (use_raw):\n",
    "        train(model, raw_data_train.float(), raw_lab_train.float(), optimizer, scheduler)\n",
    "        train_accuracy = evaluate(model, raw_data_train.float(), raw_lab_train.float())\n",
    "        val_accuracy = evaluate(model, raw_data_test.float(), raw_lab_test.float())\n",
    "    else:\n",
    "        loss = train(model, feat_data_train.float(), feat_lab_train.float(), optimizer, scheduler)\n",
    "        train_accuracy = evaluate(model, feat_data_train.float(), feat_lab_train.float())\n",
    "        val_accuracy = evaluate(model, feat_data_test.float(), feat_lab_test.float())\n",
    "\n",
    "    print(f\"epoch: {epoch}\\tLoss: {loss}  \\tTraining accuracy: {train_accuracy:.1f}%\\tValidation accuracy: {val_accuracy:.1f}%\")\n",
    "    \n",
    "    \n",
    "total_training_time = time.time() - start_time_training\n",
    "\n",
    "# Save Metrics\n",
    "now = datetime.now()\n",
    "\n",
    "# dd/mm/YY H:M:S\n",
    "date_and_time = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "save_metrics(\"./models/nn_metrics.csv\", date_and_time, model_name, total_training_time,\n",
    "             raw_lab_test.size(), seed, {\"Training Accuracy\":train_accuracy.item()/100, \n",
    "                                         \"Validation Accuracy\":val_accuracy.item()/100,\n",
    "                                         \"Epochs\":epochs, \"Hidden Dimensions\":hidden_dims, \n",
    "                                         \"Hidden Layers\":hidden_layers,\"Dropout\":dropout,\n",
    "                                         \"Learning Rate\":learning_rate, \"Gamma\":gamma})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d083200",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, len(raw_data_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "87a1e253",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(model,\n",
    "                  dummy_input,               # Desired inference input shape\n",
    "                  model_name + \".onnx\",      # Model export path + name\n",
    "                  export_params=True,        # Store weights with the model\n",
    "                  opset_version=9,           # Unity requires onnx 9\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['Input'],   # Used to identify layers during debugging\n",
    "                  output_names = ['Logits']  # Used to identify layers during debugging\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e396b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw Data Model\n",
    "model_name = \"dsc_sklearn_svm_raw\"\n",
    "svm_kernel = \"rbf\"\n",
    "svm_gamma = 0.15\n",
    "c_value = 8\n",
    "\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "scaler.fit(raw_data_train)\n",
    "\n",
    "raw_train = scaler.transform(raw_data_train)\n",
    "raw_test = scaler.transform(raw_data_test)\n",
    "\n",
    "model_SVM_raw = SVC(kernel=svm_kernel, C=c_value, gamma=svm_gamma)\n",
    "\n",
    "start_time_training = time.time()\n",
    "model_SVM_raw.fit(raw_train, raw_lab_train)\n",
    "total_training_time = time.time() - start_time_training\n",
    "\n",
    "lab_predict =  model_SVM_raw.predict(raw_test)\n",
    "\n",
    "accuracy_score = sklearn.metrics.accuracy_score(raw_lab_test, lab_predict)\n",
    "#print the number of misclassified samples, accuracy and complete report (using scikit learn metric tools) \n",
    "print('Number of mislabeled samples %d out of %d' % ((raw_lab_test != lab_predict).sum(),raw_lab_test.size))\n",
    "print('Accuracy:',accuracy_score)\n",
    "\n",
    "# Save Metrics\n",
    "now = datetime.now()\n",
    "\n",
    "# dd/mm/YY H:M:S\n",
    "date_and_time = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "save_metrics(\"./models/metrics.csv\", date_and_time, model_name, accuracy_score,c_value,svm_gamma,svm_kernel,raw_lab_test.size, total_training_time, seed)\n",
    "\n",
    "onnx_model_svm_raw = skl2onnx.convert_sklearn(model_SVM_raw, \n",
    "                                            initial_types=initial_type,\n",
    "                                            name=model_name, \n",
    "                                            target_opset=9,\n",
    "                                            verbose=0)\n",
    "\n",
    "saved_model = skl2onnx.helpers.onnx_helper.save_onnx_model(onnx_model_svm_raw, \"./models/\" + model_name + \".onnx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
