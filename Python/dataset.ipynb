{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "404bcd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import skl2onnx\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "from skl2onnx.common.data_types import FloatTensorType, Int64TensorType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c1337cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(data, sr=100, window_length = 256, hop_length = 128):\n",
    "    \n",
    "    extracted_features_frame = pd.DataFrame()\n",
    "    for col in data.columns:\n",
    "        zcr = librosa.feature.zero_crossing_rate(y=np.array(data[col], dtype=np.float64),\n",
    "                                                                        frame_length=window_length, \n",
    "                                                                        hop_length=hop_length)\n",
    "    \n",
    "        rms = librosa.feature.rms(y=np.array(data[col], dtype=np.float64),\n",
    "                                                        frame_length=window_length, \n",
    "                                                        hop_length=hop_length)\n",
    "        \n",
    "        extracted_features_frame[col + \" RMS\"] = rms.squeeze()\n",
    "        extracted_features_frame[col + \" ZCR\"] = zcr.squeeze()\n",
    "    \n",
    "    return extracted_features_frame\n",
    "\n",
    "\n",
    "def save_metrics(metrics_path,date_and_time,model_name,accuracy_score,c_val,gamma,kernel,rows,time,seed):\n",
    "    metrics_dict = {\n",
    "    'Date Time': date_and_time,\n",
    "    'Name': model_name, \n",
    "    'Accuracy': accuracy_score, \n",
    "    'C': c_val,\n",
    "    'Gamma': gamma,\n",
    "    'Kernel': kernel,\n",
    "    'Rows': rows,\n",
    "    'Training Time': total_training_time,\n",
    "    'Seed': seed\n",
    "    }    \n",
    "\n",
    "    metrics_frame = pd.DataFrame(columns=[\n",
    "        'Date Time',\n",
    "        'Name', \n",
    "        'Accuracy', \n",
    "        'C',\n",
    "        'Gamma',\n",
    "        'Kernel',\n",
    "        'Rows',\n",
    "        'Training Time', \n",
    "        'Seed'\n",
    "        ])\n",
    "\n",
    "    # Check if metrics csv exists\n",
    "    metrics_frame = pd.DataFrame(metrics_dict, index=[0])\n",
    "\n",
    "    if (os.path.exists(metrics_path)):\n",
    "        df = pd.read_csv(metrics_path, index_col=0)\n",
    "        df = pd.concat([df, metrics_frame], ignore_index=True)\n",
    "        df.to_csv(metrics_path)\n",
    "\n",
    "    else:\n",
    "        metrics_frame.to_csv(metrics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4fb8211",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/\"\n",
    "\n",
    "# PRE PROCESSING\n",
    "\n",
    "# These are irrelevant for the current task due to being idle. We remove them to speed up the training process.\n",
    "cols_to_ignore = [\"Timestamp\", \"D-Pad\", \"Touch\", \"L3\", \"R3\", \"L1\", \"R1\", \"L2\"]\n",
    "\n",
    "# Filter out slow start and finish, ensure same size.\n",
    "start_slice = 500\n",
    "end_slice = 5500\n",
    "\n",
    "labels_dict = {\n",
    "    0 : \"idle\",\n",
    "    1 : \"low_activity\",\n",
    "    2 : \"medium_activity\",\n",
    "    3 : \"high_activity\"\n",
    "}\n",
    "\n",
    "# Raw Data\n",
    "idle_frame_raw = pd.read_csv(data_path + \"idle.csv\").iloc[start_slice:end_slice]\n",
    "idle_labels_raw = np.zeros(idle_frame_raw.shape[0])\n",
    "\n",
    "low_activity_frame_raw = pd.read_csv(data_path + \"low_activity.csv\").iloc[start_slice:end_slice]\n",
    "low_activity_labels_raw = np.zeros(low_activity_frame_raw.shape[0]) + 1\n",
    "\n",
    "medium_activity_frame_raw = pd.read_csv(data_path + \"medium_activity.csv\").iloc[start_slice:end_slice]\n",
    "medium_activity_labels_raw = np.zeros(medium_activity_frame_raw.shape[0]) + 2\n",
    "\n",
    "high_activity_frame_raw = pd.read_csv(data_path + \"high_activity.csv\").iloc[start_slice:end_slice]\n",
    "high_activity_labels_raw = np.zeros(high_activity_frame_raw.shape[0]) + 3\n",
    "\n",
    "# Feature Extract\n",
    "idle_frame_features = feature_extraction(idle_frame_raw)\n",
    "idle_labels_features = np.zeros(idle_frame_features.shape[0])\n",
    "\n",
    "low_activity_frame_features = feature_extraction(low_activity_frame_raw)\n",
    "low_activity_labels_features = np.zeros(low_activity_frame_features.shape[0]) + 1\n",
    "\n",
    "medium_activity_frame_features = feature_extraction(medium_activity_frame_raw)\n",
    "medium_activity_labels_features = np.zeros(medium_activity_frame_features.shape[0]) + 2\n",
    "\n",
    "high_activity_frame_features = feature_extraction(high_activity_frame_raw)\n",
    "high_activity_labels_features = np.zeros(high_activity_frame_features.shape[0]) + 3\n",
    "\n",
    "# Concatenate Raw Dataset\n",
    "labels_raw = np.concatenate((idle_labels_raw, low_activity_labels_raw, medium_activity_labels_raw, high_activity_labels_raw))\n",
    "data_raw = pd.concat((idle_frame_raw, low_activity_frame_raw, medium_activity_frame_raw, high_activity_frame_raw))\n",
    "data_raw.reset_index(drop=True, inplace=True)\n",
    "# Concatenate Extracted Features Dataset\n",
    "labels_features = np.concatenate((idle_labels_features, low_activity_labels_features, medium_activity_labels_features, high_activity_labels_features))\n",
    "data_features = pd.concat((idle_frame_features, low_activity_frame_features, medium_activity_frame_features, high_activity_frame_features))\n",
    "data_features.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Remove unwanted features\n",
    "for col in data_raw.columns:\n",
    "    for header in cols_to_ignore:\n",
    "        if header in col:\n",
    "            data_raw = data_raw.drop(col, axis=1)\n",
    "            \n",
    "for col in data_features.columns:\n",
    "    for header in cols_to_ignore:\n",
    "        if header in col:\n",
    "            data_features = data_features.drop(col, axis=1)\n",
    "            \n",
    "            \n",
    "# There is a leading whitespace in each header. This removes it.\n",
    "for col in data_raw.columns:\n",
    "    if (col[0] == \" \"):\n",
    "        data_raw.rename(columns={col: col.strip()}, inplace=True)\n",
    "        data_features.rename(columns={col: col.strip()}, inplace=True)\n",
    "        \n",
    "for col in data_features.columns:\n",
    "    if (col[0] == \" \"):\n",
    "        data_features.rename(columns={col: col.strip()}, inplace=True)\n",
    "        \n",
    "        \n",
    "data_features.to_csv(\"./data/data_features.csv\")\n",
    "data_raw.to_csv(\"./data/data_raw.csv\")\n",
    "pd.Series(labels_features).to_csv(\"./data/labels_features.csv\")\n",
    "pd.Series(labels_raw).to_csv(\"./data/labels_raw.csv\")\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fad63412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nraw_data_train, raw_data_train, raw_lab_train, raw_lab_test = sklearn.model_selection.train_test_split(raw_data_train,\\n                                                                          raw_lab_train,\\n                                                                          train_size=0.8, \\n                                                                          random_state=seed)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 5550\n",
    "\n",
    "# Training using features\n",
    "feat_data_train, feat_data_test, feat_lab_train, feat_lab_test = sklearn.model_selection.train_test_split(data_features,\n",
    "                                                                                                        labels_features,\n",
    "                                                                                                        train_size=0.8, \n",
    "                                                                                                        random_state=seed, \n",
    "                                                                                                        stratify=labels_features)\n",
    "'''\n",
    "feat_data_train, feat_data_val, feat_lab_train, feat_lab_val  = sklearn.model_selection.train_test_split(feat_data_train,\n",
    "                                                                          feat_lab_train,\n",
    "                                                                          train_size=0.8, \n",
    "                                                                          random_state=seed)'''\n",
    "\n",
    "raw_data_train, raw_data_test, raw_lab_train, raw_lab_test = sklearn.model_selection.train_test_split(data_raw,\n",
    "                                                                        labels_raw,\n",
    "                                                                        train_size=0.8, \n",
    "                                                                        random_state=seed, \n",
    "                                                                        stratify=labels_raw)\n",
    "\n",
    "'''\n",
    "raw_data_train, raw_data_train, raw_lab_train, raw_lab_test = sklearn.model_selection.train_test_split(raw_data_train,\n",
    "                                                                          raw_lab_train,\n",
    "                                                                          train_size=0.8, \n",
    "                                                                          random_state=seed)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "91162eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled samples 5 out of 32\n",
      "Accuracy: 0.84375\n"
     ]
    }
   ],
   "source": [
    "# Using extracted features\n",
    "model_name = \"dsc_sklearn_svm_feat\"\n",
    "svm_kernel = \"rbf\"\n",
    "svm_gamma = 0.15\n",
    "c_value = 10\n",
    "\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "scaler.fit(feat_data_train)\n",
    "\n",
    "feat_train = scaler.transform(feat_data_train)\n",
    "feat_test = scaler.transform(feat_data_test)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "model_SVM = SVC(kernel='rbf', C=10, gamma=0.1)\n",
    "\n",
    "#training the model\n",
    "start_time_training = time.time()\n",
    "model_SVM.fit(feat_train, feat_lab_train)\n",
    "total_training_time = time.time() - start_time_training\n",
    "\n",
    "#applying the the model on the test data (features)\n",
    "lab_predict =  model_SVM.predict(feat_test)\n",
    "\n",
    "accuracy_score = sklearn.metrics.accuracy_score(feat_lab_test, lab_predict)\n",
    "#print the number of misclassified samples, accuracy and complete report (using scikit learn metric tools) \n",
    "print('Number of mislabeled samples %d out of %d' % ((feat_lab_test != lab_predict).sum(),feat_lab_test.size))\n",
    "print('Accuracy:',sklearn.metrics.accuracy_score(feat_lab_test, lab_predict))\n",
    "\n",
    "# Save Metrics\n",
    "now = datetime.now()\n",
    "\n",
    "# dd/mm/YY H:M:S\n",
    "date_and_time = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "save_metrics(\"./models/metrics.csv\", date_and_time, model_name, accuracy_score,c_value,svm_gamma,svm_kernel,raw_lab_test.size, total_training_time, seed)\n",
    "\n",
    "# Export ONNX for Unity\n",
    "initial_type = [('float_input', FloatTensorType([None, 500]))]\n",
    "\n",
    "onnx_model_svm_feat = skl2onnx.convert_sklearn(model_SVM, \n",
    "                                            initial_types=initial_type,\n",
    "                                            name=model_name, \n",
    "                                            target_opset=9,\n",
    "                                            verbose=0)\n",
    "\n",
    "saved_model = skl2onnx.helpers.onnx_helper.save_onnx_model(onnx_model_svm_feat, model_name + \".onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9e396b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled samples 1376 out of 4000\n",
      "Accuracy: 0.656\n"
     ]
    }
   ],
   "source": [
    "# Raw Data Model\n",
    "model_name = \"dsc_sklearn_svm_raw\"\n",
    "svm_kernel = \"rbf\"\n",
    "svm_gamma = 0.15\n",
    "c_value = 8\n",
    "\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "scaler.fit(raw_data_train)\n",
    "\n",
    "raw_train = scaler.transform(raw_data_train)\n",
    "raw_test = scaler.transform(raw_data_test)\n",
    "\n",
    "model_SVM_raw = SVC(kernel=svm_kernel, C=c_value, gamma=svm_gamma)\n",
    "\n",
    "start_time_training = time.time()\n",
    "model_SVM_raw.fit(raw_train, raw_lab_train)\n",
    "total_training_time = time.time() - start_time_training\n",
    "\n",
    "lab_predict =  model_SVM_raw.predict(raw_test)\n",
    "\n",
    "accuracy_score = sklearn.metrics.accuracy_score(raw_lab_test, lab_predict)\n",
    "#print the number of misclassified samples, accuracy and complete report (using scikit learn metric tools) \n",
    "print('Number of mislabeled samples %d out of %d' % ((raw_lab_test != lab_predict).sum(),raw_lab_test.size))\n",
    "print('Accuracy:',accuracy_score)\n",
    "\n",
    "# Save Metrics\n",
    "now = datetime.now()\n",
    "\n",
    "# dd/mm/YY H:M:S\n",
    "date_and_time = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "save_metrics(\"./models/metrics.csv\", date_and_time, model_name, accuracy_score,c_value,svm_gamma,svm_kernel,raw_lab_test.size, total_training_time, seed)\n",
    "\n",
    "onnx_model_svm_raw = skl2onnx.convert_sklearn(model_SVM_raw, \n",
    "                                            initial_types=initial_type,\n",
    "                                            name=model_name, \n",
    "                                            target_opset=9,\n",
    "                                            verbose=0)\n",
    "\n",
    "saved_model = skl2onnx.helpers.onnx_helper.save_onnx_model(onnx_model_svm_raw, \"./models/\" + model_name + \".onnx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
