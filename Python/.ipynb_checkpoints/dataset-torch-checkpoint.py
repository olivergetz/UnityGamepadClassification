{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "404bcd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import sklearn.model_selection\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from datetime import datetime\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "from torchmetrics.classification import MulticlassPrecision\n",
    "from torchmetrics.classification import MulticlassRecall\n",
    "from torchmetrics.classification import MulticlassF1Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bd776e",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "Easy access to most parameters one would want to change during experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7281add6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This seed is used to seed everything seedable.\n",
    "seed = 4052\n",
    "# The path where the data was recorded.\n",
    "data_path = \"../CollectedData/Dualsense/\"\n",
    "metrics_path = \"./models/systematic_metrics.csv\"\n",
    "best_model_paths = \"./models/systematic/\"\n",
    "\n",
    "# The portion of the dataset used for training.\n",
    "training_size = 0.5\n",
    "# Since the window is what we are using to make the computation, \n",
    "# The buffer in Unity during inference should be the same value.\n",
    "window_length_feature_extraction = 256\n",
    "# Since features are computed constantly in Unity, low hop might be best.\n",
    "hop_length_feature_extraction = 16\n",
    "\n",
    "# Select dataset\n",
    "current_dataset = 4\n",
    "# Name, Sample Rate\n",
    "dataset_info = [\n",
    "    ['Guided', 100], \n",
    "    ['Guided_Exaggerated', 100], \n",
    "    ['Guided_Highres', 200], \n",
    "    ['Guided_Long', 100], \n",
    "    ['Guided_Long_Highres', 300], \n",
    "    ['No_Guide', 100],\n",
    "    ['No_Guide_Exaggerated', 300],\n",
    "    ['Separation_Test', 200]\n",
    "]\n",
    "\n",
    "current_metric = 0\n",
    "metric_names = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "model_name = \"dsc_sys-\" + dataset_info[current_dataset][0].lower()\n",
    "\n",
    "# Percentage of data to filter out from start and finish to account for slow start.\n",
    "filter_portion = 0.02\n",
    "\n",
    "# These are irrelevant for the current task due to being idle. \n",
    "# We remove them to speed up the training process.\n",
    "cols_to_ignore = [\"Timestamp\", \"D-Pad\", \"Touch\", \"L3\", \"R3\", \"L2\",\n",
    "       'Button North', 'Button East',]\n",
    "\n",
    "# Exclude Mean, Var, or RMS for testing, or leave empty to keep all features.\n",
    "# Remember to set the appropriate bools in Unity.\n",
    "feats_to_ignore = [\"Var\"]\n",
    "\n",
    "# Model Hyperparameters\n",
    "epochs = 10\n",
    "hidden_layers = 4\n",
    "hidden_dims = 32\n",
    "dropout = 0.7\n",
    "learning_rate = 5e-8\n",
    "max_learning_rate = 5e-5\n",
    "gamma=0.9 # Not used for the final model. Used for experiments with other optimizers.\n",
    "use_raw = False; # Want to train on the raw data, or the extracted features?\n",
    "early_stop_patience = 5\n",
    "early_stop_thresh = 0.0005\n",
    "\n",
    "# All unique class labels. Used for debugging predictions.\n",
    "# Order alphabetically.\n",
    "labels_dict = {\n",
    "    0 : \"high_activity\",\n",
    "    1 : \"idle\",\n",
    "    2 : \"low_activity\",\n",
    "    3 : \"medium_activity\" \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f22153",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This section includes class and function definitions. The early stopper is used to prevent the models from overfitting and speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85872888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Definitions\n",
    "\n",
    "class DSC_Classifier(nn.Module):\n",
    "    def __init__(self, n_feats, n_labels, n_hidden_layers, hidden_dims, dropout):\n",
    "        super().__init__()\n",
    "        self.input = nn.Linear(n_feats, hidden_dims, bias=True)\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_dims, hidden_dims, bias=True)\n",
    "            for _ in range(n_hidden_layers)\n",
    "        ])\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dims, n_labels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input(x).relu()\n",
    "        for layer in self.hidden_layers:\n",
    "            x = x + layer(x).relu()\n",
    "            \n",
    "        logits = self.output(x)\n",
    "        return logits\n",
    "        \n",
    "\n",
    "class EarlyStopper:\n",
    "    # patience: Max consecutive epochs of no improvement\n",
    "    # learn_thresh: the amount of improvement needed to count as learning\n",
    "    def __init__(self, patience : int = 3, learn_thresh : float = 0.001, gl_threshold = 50):\n",
    "        # GL Stopper\n",
    "        self.generalization_loss = 0\n",
    "        self.lowest_loss = 100\n",
    "        self.gl_threshold = gl_threshold\n",
    "        \n",
    "        # Metric stopper\n",
    "        self.patience = patience\n",
    "        self.elapsed_epochs = 0\n",
    "        self.best_score = 0\n",
    "        self.learn_thresh = learn_thresh\n",
    "        \n",
    "    def generalization_stop(self, validation_loss : float):\n",
    "        # Update lowest validation set error\n",
    "        if (validation_loss < self.lowest_loss): self.lowest_loss = validation_loss\n",
    "        \n",
    "        # GL = The relative increase in loss over min loss, expressed as %.\n",
    "        #self.generalization_loss = 100*(validation_loss / self.lowest_loss - 1) \n",
    "        self.generalization_loss = 100 * (validation_loss - self.lowest_loss) / self.lowest_loss\n",
    "        print(self.generalization_loss)\n",
    "        if (self.generalization_loss > self.gl_threshold):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def get_generalization_loss(self):\n",
    "        return self.generalization_loss\n",
    "            \n",
    "    # This is a stopper used to stop when the supplied metric is no longer improving.\n",
    "    def stop_check(self, metric_current_score : float):\n",
    "        if metric_current_score > self.best_score:\n",
    "            self.best_score = metric_current_score\n",
    "            self.elapsed_epochs = 0 \n",
    "        elif metric_current_score < (self.best_score - self.learn_thresh):\n",
    "            self.elapsed_epochs += 1\n",
    "            if self.elapsed_epochs >= self.patience:\n",
    "                print(\"Stopped by EarlyStopper.\")\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c1337cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Definitions\n",
    "def seed_everything(seed_value=4052):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def feature_extraction(data, sr=100, window_length = 256, hop_length = 64):\n",
    "\n",
    "    extracted_features_frame = pd.DataFrame()\n",
    "    for col in data.columns:\n",
    "        # Convert to numpy for easy indexing.\n",
    "        data_np = data[col].values\n",
    "\n",
    "        '''zcr = librosa.feature.zero_crossing_rate(y=np.array(data[col], dtype=np.float64),\n",
    "                                                                        frame_length=window_length, \n",
    "                                                                        hop_length=hop_length)'''\n",
    "    \n",
    "        rms = librosa.feature.rms(y=np.array(data[col], dtype=np.float64),\n",
    "                                frame_length=window_length, \n",
    "                                hop_length=hop_length)\n",
    "        \n",
    "        \n",
    "        # Init lists\n",
    "        variance = []\n",
    "        mean = []\n",
    "        # Sliding window mean and variance\n",
    "        for i in range(data_np.size // hop_length):\n",
    "            # Get current window\n",
    "            current_position = i * hop_length\n",
    "            window_end_idx = current_position+window_length\n",
    "            current_window = data_np[current_position:window_end_idx]\n",
    "            # Calculate features and add them to their respective lists\n",
    "            window_mean = np.mean(current_window)\n",
    "            mean.append(window_mean)\n",
    "            window_variance = np.var(current_window)\n",
    "            variance.append(window_variance)\n",
    "        \n",
    "        # Make sure all other features match the length of the first feature\n",
    "        features_length = len(mean)\n",
    "        extracted_features_frame[col + \" Mean\"] = mean\n",
    "        extracted_features_frame[col + \" Var\"] = variance\n",
    "\n",
    "        # librosa returns array of size (0, n). Remove the first dimension.\n",
    "        rms = rms.squeeze()\n",
    "        # Cut or pad features to an appropriate length\n",
    "        if(rms.size > features_length):\n",
    "            diff = np.absolute(rms.size-features_length)\n",
    "            rms = rms[:-diff]\n",
    "        elif (rms.size < features_length):\n",
    "            diff = np.absolute(rms.size-features_length)\n",
    "            rms = np.append(rms, np.zeros(diff))\n",
    "            \n",
    "        extracted_features_frame[col + \" RMS\"] = rms\n",
    "        \n",
    "        #extracted_features_frame[col + \" ZCR\"] = zcr.squeeze()\n",
    "    \n",
    "    return extracted_features_frame\n",
    "\n",
    "def train(model, x, y, optimizer, scheduler):\n",
    "    model.train()\n",
    "    for feature_vector, label_true in zip(x, y):\n",
    "        optimizer.zero_grad()\n",
    "        label_pred = model(feature_vector)\n",
    "        loss = nn.functional.cross_entropy(label_pred, label_true.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    return loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, x, y, n_classes):\n",
    "    model.eval()\n",
    "    labels_true, predictions = [], []\n",
    "    for feature_vector, label_true in zip(x, y):\n",
    "        output = model(feature_vector)\n",
    "        predictions.append(output.argmax().tolist())\n",
    "        labels_true.append(label_true.tolist())\n",
    "        loss = nn.functional.cross_entropy(output, label_true.long())\n",
    "        \n",
    "    # Convert to tensors\n",
    "    preds = torch.tensor(predictions)\n",
    "    targets = torch.tensor(labels_true)\n",
    "    \n",
    "    #accuracy = (torch.tensor(predictions) == torch.tensor(labels_true)).float().mean() * 100.0\n",
    "    accuracy = MulticlassAccuracy(average='macro', num_classes=n_classes)\n",
    "    acc = accuracy(preds, targets)\n",
    "    precision = MulticlassPrecision(average='macro', num_classes=n_classes)\n",
    "    prec = precision(preds, targets)\n",
    "    recall = MulticlassRecall(average='macro', num_classes=n_classes)\n",
    "    rec = recall(preds, targets)\n",
    "    f1_score = MulticlassF1Score(average='macro', num_classes=n_classes)\n",
    "    f1 = f1_score(preds, targets)\n",
    "    return acc, prec, rec, f1, loss\n",
    "\n",
    "def save_metrics(metrics_path:str,date_and_time,model_name:str,time,\n",
    "                 data_size:int,train_set:float,seed:int, metrics_dict:dict):\n",
    "    # Info common to all metrics\n",
    "    meta_data = {\n",
    "    'Date Time': date_and_time,\n",
    "    'Name': model_name, \n",
    "    'Training Time': total_training_time,\n",
    "    'Dataset Size': data_size,\n",
    "    'Train Set': train_set,\n",
    "    'Seed': seed\n",
    "    }\n",
    "    \n",
    "    # Merge Metadata with Model-specific metrics\n",
    "    metrics_dict = meta_data | metrics_dict\n",
    "    \n",
    "    # Convert to data frame\n",
    "    metrics_frame = pd.DataFrame(columns=metrics_dict.keys())\n",
    "\n",
    "    # Check if metrics csv exists\n",
    "    metrics_frame = pd.DataFrame(metrics_dict, index=[0])\n",
    "\n",
    "    if (os.path.exists(metrics_path)):\n",
    "        df = pd.read_csv(metrics_path, index_col=0)\n",
    "        df = pd.concat([df, metrics_frame], ignore_index=True)\n",
    "        df.to_csv(metrics_path)\n",
    "\n",
    "    else:\n",
    "        metrics_frame.to_csv(metrics_path)\n",
    "        \n",
    "def get_best_metric(path, key):\n",
    "    if (os.path.exists(metrics_path)):\n",
    "        df = pd.read_csv(metrics_path, index_col=0)\n",
    "        max_vals = df.max(numeric_only=True)\n",
    "        return max_vals[key]\n",
    "\n",
    "    else:\n",
    "        print(f\"Could not find {path}. Returning 0.\")\n",
    "        # Returning 0 so the metric will always be better if the file does not exist.\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8ac09e",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "This part handles and processes the dataset. Features not useful for learning are discarded. The data is converted to separate dataframe objects and features are computed. A new .csv file with extracted features is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e362d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels order:\n",
      "Label 0 = High_Activity_Guided_Long_Hires.csv\n",
      "Label 1 = Idle_Guided_Long_Hires.csv\n",
      "Label 2 = Low_Activity_Guided_Long_Hires.csv\n",
      "Label 3 = Medium_Activity_Guided_Long_Hires.csv\n"
     ]
    }
   ],
   "source": [
    "seed_everything(seed_value=seed)\n",
    "\n",
    "data_raw = pd.DataFrame()\n",
    "labels_raw = np.array([])\n",
    "data_features = pd.DataFrame()\n",
    "labels_features = np.array([])\n",
    "# Extract features for the current dataset, create index labels, and combine .csv files.\n",
    "print(\"Labels order:\")\n",
    "for root, dirs, files in os.walk(data_path):\n",
    "        if (root.endswith(dataset_info[current_dataset][0])):\n",
    "            current_dataset_name = dataset_info[current_dataset][0]\n",
    "            \n",
    "            # Iterate over files that end with .csv\n",
    "            files_iter = filter(lambda x: x.endswith('.csv'), files)\n",
    "            \n",
    "            for i, file in enumerate(files_iter):         \n",
    "                    #if \"Idle\" in file: continue # Used for testing.\n",
    "                    \n",
    "                    labels_dict[i] = file.split('.')[0] # Align index and label.\n",
    "                    \n",
    "                    # Raw Data\n",
    "                    csv_file_path = data_path + dataset_info[current_dataset][0] + \"/\" + file\n",
    "                    dataset_frame = pd.read_csv(csv_file_path)\n",
    "                    # Filter out slow start and finish - Removes error values from writing files.\n",
    "                    slice_amt = (int)(dataset_frame.shape[0] * filter_portion)\n",
    "                    dataset_frame = dataset_frame.iloc[slice_amt:-slice_amt]\n",
    "                    # Create aggregated data sets\n",
    "                    labels_raw = np.append(labels_raw, (np.zeros(dataset_frame.shape[0]) + i), axis=0)\n",
    "                    data_raw = pd.concat((data_raw, dataset_frame))\n",
    "                    data_raw.reset_index(drop = True, inplace = True)\n",
    "                    \n",
    "                    # Feature Data\n",
    "                    extracted_feats = feature_extraction(\n",
    "                                dataset_frame, \n",
    "                                # Set sample rate to the value that was used during recording.\n",
    "                                sr = dataset_info[current_dataset][1],\n",
    "                                window_length = window_length_feature_extraction, \n",
    "                                hop_length = hop_length_feature_extraction)\n",
    "                    # Aggregated fatures from all classes\n",
    "                    data_features = pd.concat((data_features, extracted_feats))\n",
    "                    # Ensure df indeces ranges 0...n\n",
    "                    data_features.reset_index(drop = True, inplace = True) \n",
    "                    current_label_feat = np.zeros(extracted_feats.shape[0]) + i\n",
    "                    labels_features = np.append(labels_features, current_label_feat, axis=0)\n",
    "                    # Inspect label alignment:\n",
    "                    print(f\"Label {i} = {file}\")\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4fb8211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unwanted features\n",
    "for col in data_raw.columns:\n",
    "    for header in cols_to_ignore:\n",
    "        if header in col:\n",
    "            data_raw = data_raw.drop(col, axis=1)\n",
    "\n",
    "# Remove columns not relevant for training (i.e. Timestamp)\n",
    "for col in data_features.columns:\n",
    "    for header in cols_to_ignore:\n",
    "        if header in col:\n",
    "            data_features = data_features.drop(col, axis=1)\n",
    "\n",
    "# Remove extracted features, if any\n",
    "for col in data_features.columns:\n",
    "    for header in feats_to_ignore:\n",
    "        if header in col:\n",
    "            data_features = data_features.drop(col, axis=1)\n",
    "                \n",
    "# There is a leading whitespace in each header. This removes it.\n",
    "for col in data_raw.columns:\n",
    "    if (col[0] == \" \"):\n",
    "        data_raw.rename(columns={col: col.strip()}, inplace=True)\n",
    "        data_features.rename(columns={col: col.strip()}, inplace=True)\n",
    "        \n",
    "for col in data_features.columns:\n",
    "    if (col[0] == \" \"):\n",
    "        data_features.rename(columns={col: col.strip()}, inplace=True)\n",
    "\n",
    "        \n",
    "# Save features. \n",
    "processed_data_path = \"./data/\" + model_name\n",
    "data_features.to_csv(processed_data_path + \"_data_features.csv\")\n",
    "data_raw.to_csv(processed_data_path + \"_data_raw.csv\")\n",
    "pd.Series(labels_features).to_csv(processed_data_path + \"_labels_features.csv\")\n",
    "pd.Series(labels_raw).to_csv(processed_data_path + \"_labels_raw.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f11ad6",
   "metadata": {},
   "source": [
    "## Data Inspection\n",
    "\n",
    "This section is used to get a sense of which features may be able to separate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48f1abc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(data_features.columns)\\n%matplotlib widget\\n#%matplotlib notebook\\nfrom mpl_toolkits.mplot3d import Axes3D\\n\\nfig = plt.figure()\\nax = fig.add_subplot(111, projection =\"3d\")\\n\\nax.scatter3D(data_features[\\'Accelerometer X Mean\\'], \\n            data_features[\\'Accelerometer Y Mean\\'], \\n            data_features[\\'Accelerometer Z Mean\\'])\\n\\nax.scatter3D(data_features[\\'Accelerometer X Var\\'], \\n            data_features[\\'Accelerometer Y Var\\'], \\n            data_features[\\'Accelerometer Z Var\\'])\\n\\nax.set_xlabel(\\'Axis 1\\')\\nax.set_ylabel(\\'Axis 2\\')\\nax.set_zlabel(\\'Axis 3\\')\\n\\nplt.show()'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "print(data_features.columns)\n",
    "%matplotlib widget\n",
    "#%matplotlib notebook\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection =\"3d\")\n",
    "\n",
    "ax.scatter3D(data_features['Accelerometer X Mean'], \n",
    "            data_features['Accelerometer Y Mean'], \n",
    "            data_features['Accelerometer Z Mean'])\n",
    "\n",
    "ax.scatter3D(data_features['Accelerometer X Var'], \n",
    "            data_features['Accelerometer Y Var'], \n",
    "            data_features['Accelerometer Z Var'])\n",
    "\n",
    "ax.set_xlabel('Axis 1')\n",
    "ax.set_ylabel('Axis 2')\n",
    "ax.set_zlabel('Axis 3')\n",
    "\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67819d04",
   "metadata": {},
   "source": [
    "## Data Split\n",
    "\n",
    "Converting data to tensors and splitting into train and dev sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fad63412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensor\n",
    "data_raw_tensor = torch.tensor([data_raw[col].astype('float64') for col in data_raw.columns]).T.double()\n",
    "lab_raw_tensor = torch.tensor(labels_raw).double()\n",
    "\n",
    "data_features_tensor = torch.tensor([data_features[col].astype('float64') for col in data_features.columns]).T.double()\n",
    "lab_features_tensor = torch.tensor(labels_features).double()\n",
    "\n",
    "# For training using features\n",
    "feat_data_train, feat_data_test, feat_lab_train, feat_lab_test = sklearn.model_selection.train_test_split(\n",
    "                                                                    data_features_tensor,\n",
    "                                                                    lab_features_tensor,\n",
    "                                                                    train_size=training_size, \n",
    "                                                                    random_state=seed,\n",
    "                                                                    stratify=labels_features)  \n",
    "\n",
    "# For training using raw data\n",
    "raw_data_train, raw_data_test, raw_lab_train, raw_lab_test = sklearn.model_selection.train_test_split(\n",
    "                                                                data_raw_tensor,\n",
    "                                                                lab_raw_tensor,\n",
    "                                                                train_size=training_size, \n",
    "                                                                random_state=seed, \n",
    "                                                                stratify=labels_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a918ba",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "This section contains the training portion of the project, along with exporting metrics for record keeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91162eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tTrain Loss: 1.606315016746521 \tValidation Loss: 1.5122802257537842  \tAccuracy: 0.260\n",
      "Epoch: 1\tTrain Loss: 1.2752387523651123 \tValidation Loss: 1.29542875289917  \tAccuracy: 0.305\n",
      "Epoch: 2\tTrain Loss: 1.2105741500854492 \tValidation Loss: 0.9882233142852783  \tAccuracy: 0.516\n",
      "Epoch: 3\tTrain Loss: 0.8143421411514282 \tValidation Loss: 0.7227907776832581  \tAccuracy: 0.900\n",
      "Epoch: 4\tTrain Loss: 0.5146056413650513 \tValidation Loss: 0.600462794303894  \tAccuracy: 0.855\n",
      "Epoch: 5\tTrain Loss: 1.3490362167358398 \tValidation Loss: 0.5714539885520935  \tAccuracy: 0.912\n",
      "Epoch: 6\tTrain Loss: 0.700194776058197 \tValidation Loss: 0.5213244557380676  \tAccuracy: 0.931\n"
     ]
    }
   ],
   "source": [
    "# Create a model using either the raw data or features\n",
    "if (use_raw):\n",
    "    torch.manual_seed(seed)\n",
    "    model = DSC_Classifier(len(raw_data_train[1]), len(labels_dict), hidden_layers, hidden_dims, dropout) # 16000, 8, 4\n",
    "else:\n",
    "    torch.manual_seed(seed)\n",
    "    model = DSC_Classifier(len(feat_data_train[1]), len(labels_dict), hidden_layers, hidden_dims, dropout) # 128, 8, 4\n",
    "\n",
    "# This stops training early if a certain criteria is met.\n",
    "early_stopper = EarlyStopper(patience=early_stop_patience, learn_thresh=early_stop_thresh)\n",
    "#early_stopper = EarlyStopper(gl_threshold = 50)\n",
    "    \n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.0) # Causes no learning.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr = max_learning_rate, epochs=epochs, steps_per_epoch=4096)\n",
    "\n",
    "best_metric_score = 0\n",
    "start_time_training = time.time()\n",
    "for epoch in range(epochs):\n",
    "    if (use_raw):\n",
    "        train(model, raw_data_train.float(), raw_lab_train.float(), optimizer, scheduler)\n",
    "        accuracy, precision, recall, f1, train_loss = evaluate(\n",
    "            model, \n",
    "            raw_data_test.float(), \n",
    "            raw_lab_test.float(),\n",
    "            len(labels_dict)\n",
    "        )\n",
    "    else:\n",
    "        loss = train(model, feat_data_train.float(), feat_lab_train.float(), optimizer, scheduler)\n",
    "        accuracy, precision, recall, f1, val_loss = evaluate(\n",
    "            model, \n",
    "            feat_data_test.float(), \n",
    "            feat_lab_test.float(),\n",
    "            len(labels_dict)\n",
    "        )\n",
    "    \n",
    "    # Set focus metric to which ever metric we currently find important.\n",
    "    # Used to avoid having to duplicate code when saving.\n",
    "    if(metric_names[current_metric] == 'accuracy'): focus_metric = accuracy\n",
    "    if(metric_names[current_metric] == 'precision'): focus_metric = precision\n",
    "    if(metric_names[current_metric] == 'recall'): focus_metric = recall\n",
    "    if(metric_names[current_metric] == 'f1'): focus_metric = f1\n",
    "    \n",
    "\n",
    "    # Save the current data if its better than earlier models.\n",
    "    if (focus_metric > best_metric_score): \n",
    "        best_metric_score = focus_metric\n",
    "        model_specific_metrics = {\n",
    "        \"Mean\": any([\"Mean\" in col.split(\" \") for col in data_features.columns]),\n",
    "        \"Variance\": any([\"Var\" in col.split(\" \") for col in data_features.columns]),\n",
    "        \"RMS\": any([\"RMS\" in col.split(\" \") for col in data_features.columns]),\n",
    "        \"Frame Size\": window_length_feature_extraction,\n",
    "        \"Hop Length\": hop_length_feature_extraction,\n",
    "        \"Accuracy\":accuracy.item(), \n",
    "        \"Precision\":precision.item(),\n",
    "        \"Recall\":recall.item(),\n",
    "        \"F1\":f1.item(),\n",
    "        \"Epochs\":epoch, \n",
    "        \"Hidden Dimensions\":hidden_dims, \n",
    "        \"Hidden Layers\":hidden_layers,\n",
    "        \"Dropout\":dropout,\n",
    "        \"Learning Rate\":learning_rate, \n",
    "        \"Max Learning Rate\":max_learning_rate\n",
    "        }\n",
    "        \n",
    "        # Save the model if the current metric is better than previous models.\n",
    "        #best_recorded_metric = get_best_metric(metrics_path, metric_names[current_metric].title())\n",
    "        #if (focus_metric > best_recorded_metric):\n",
    "            #print(f\"New best model found for {metric_names[current_metric]}: {focus_metric}\")\n",
    "            \n",
    "        #out_model_name = model_name + \"_best_\" + metric_names[current_metric]\n",
    "        out_model_name = model_name\n",
    "        # The dummy input shows the model how the real input should be formatted.\n",
    "        dummy_input = torch.randn(1, len(feat_data_train[1]))\n",
    "\n",
    "        # Export model in ONNX format.\n",
    "        torch.onnx.export(model,\n",
    "                          dummy_input,               # Desired inference input shape\n",
    "                          best_model_paths + out_model_name + \".onnx\",  # Model export path + name\n",
    "                          export_params=True,        # Store weights with the model\n",
    "                          opset_version=9,           # Unity requires onnx 9\n",
    "                          do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                          input_names = ['Input'],   # Used to identify layers during debugging\n",
    "                          output_names = ['Logits']  # Used to identify layers during debugging\n",
    "                          )\n",
    "        \n",
    "    print(f'Epoch: {epoch}\\tTrain Loss: {loss} \\tValidation Loss: {val_loss}  \\t{metric_names[current_metric].title()}: {focus_metric:.3f}')\n",
    "    #print(f'\\t\\t  Generalization Loss: {early_stopper.get_generalization_loss():.3f}')\n",
    "    \n",
    "    # If the main metric is not improving, stop learning.\n",
    "    if(early_stopper.stop_check(focus_metric)):\n",
    "        model_specific_metrics[\"Epochs\"] = epoch\n",
    "        break\n",
    "        \n",
    "    #if(early_stopper.generalization_stop(val_loss)):\n",
    "        #model_specific_metrics[\"Epochs\"] = epoch\n",
    "        #break\n",
    "    \n",
    "\n",
    "total_training_time = time.time() - start_time_training\n",
    "\n",
    "# Save Metrics\n",
    "now = datetime.now()\n",
    "\n",
    "# DateTime formatting\n",
    "date_and_time = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "save_metrics(\n",
    "    metrics_path, \n",
    "    date_and_time, \n",
    "    model_name, \n",
    "    total_training_time,\n",
    "    feat_lab_train.shape[0], \n",
    "    training_size,\n",
    "    seed, \n",
    "    model_specific_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88789ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
